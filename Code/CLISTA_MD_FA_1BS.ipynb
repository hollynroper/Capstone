{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3729,"status":"ok","timestamp":1680192840364,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"},"user_tz":300},"id":"IfDmYam1QCKN"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import keras as keras\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":142,"status":"ok","timestamp":1680192840496,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"},"user_tz":300},"id":"KKhMvODWQCmc"},"outputs":[],"source":["# define problem parameters\n","n = 270\n","N = 1024\n","k = 40\n","\n","num_layers = 15\n","\n","# define number of training/validation/testing samples\n","num_train_samples = 75000\n","num_valid_samples = 25000\n","num_test_samples = 25000\n","total_num_samples = num_train_samples + num_valid_samples + num_test_samples\n","\n","# use default rng\n","rng = np.random.default_rng()\n","\n","# system parameters\n","dual_bs = False\n","fading = True\n","noise = True\n","cmplx = True ## changing this to False won't do anything\n","snr_dB = [-12, -10, -5, 1, 5, 10, 20]\n","num_layers = 15"]},{"cell_type":"markdown","metadata":{"id":"DEiKFLXGmeLI"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHppsmAqQEkh"},"outputs":[],"source":["# defining functions\n","def tf_cmplx_eta(u,T):\n","  return tf.math.exp(tf.complex(0.0,tf.math.angle(u))) * tf.complex(tf.math.maximum(tf.math.abs(u)-T, 0),0.0)\n","\n","class CRF_LISTALayer(keras.layers.Layer):\n","  \n","  # defining variables for the layer\n","  def __init__(self, A, L, init_alpha, Q, W):\n","    super().__init__()\n","    self.A = A\n","    self.L = L\n","    self.n, self.N = self.A.shape\n","    self.alpha = tf.Variable(initial_value=init_alpha, dtype='float32', trainable=True)\n","    self.Q = tf.Variable(initial_value=init_Q, dtype='complex64', trainable=True)\n","    self.W = tf.Variable(initial_value=init_W, dtype='complex64', trainable=True)\n","\n","  # creating inputs and updating things on each call\n","  def call(self, inputs):\n","    xHt = inputs[:, 0:self.N]\n","    y = inputs[:, self.N:]\n","    update = xHt @ self.Q + y @ self.W\n","    new_x = tf_cmplx_eta(update, self.alpha/self.L)  # this is what makes it differ from other LISTA layer\n","    return tf.concat([new_x, y], axis=1)\n","\n","class CustomMSELoss(keras.losses.Loss):\n","\n","  def __init__(self, N):\n","    super().__init__()\n","    self.N = N\n","  \n","  def call(self, true, pred):\n","    x_true = true[:, 0:self.N]\n","    x_pred = pred[:, 0:self.N]\n","    return tf.reduce_mean(tf.math.squared_difference(tf.complex(x_true,0.0), x_pred))\n","    #return tf.reduce_mean(tf.square(tf.abs(tf.complex(x_true,0.0) - x_pred)))\n","  \n","class CustomMSEMetric(keras.metrics.Mean):\n","\n","  def __init__(self, N):\n","    super().__init__()\n","    self.N = N\n","\n","  def update_state(self, true, pred, sample_weight=None):\n","    x_true = true[:, 0:self.N]\n","    x_pred = pred[:, 0:self.N]\n","    val = tf.reduce_mean(tf.math.squared_difference(tf.complex(x_true,0.0), x_pred))\n","    #val = tf.reduce_mean(tf.square(tf.abs(tf.complex(x_true,0.0) - x_pred)))\n","    return super().update_state(val, sample_weight=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUw_eJW7P77E","executionInfo":{"status":"ok","timestamp":1680186090181,"user_tz":300,"elapsed":3273102,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"}},"outputId":"66d0b16d-23c2-48bf-d29b-fad171057a74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","586/586 [==============================] - 19s 17ms/step - loss: 14.6107 - mean: 14.6092 - val_loss: 0.4969 - val_mean: 0.4971\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2629 - mean: 0.2628 - val_loss: 0.1468 - val_mean: 0.1469\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.1029 - mean: 0.1029 - val_loss: 0.0754 - val_mean: 0.0754\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0627 - mean: 0.0627 - val_loss: 0.0547 - val_mean: 0.0547\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0480 - mean: 0.0480 - val_loss: 0.0444 - val_mean: 0.0444\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0403 - mean: 0.0403 - val_loss: 0.0387 - val_mean: 0.0387\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0357 - mean: 0.0357 - val_loss: 0.0349 - val_mean: 0.0349\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0326 - mean: 0.0326 - val_loss: 0.0320 - val_mean: 0.0320\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0303 - mean: 0.0303 - val_loss: 0.0303 - val_mean: 0.0303\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0286 - mean: 0.0286 - val_loss: 0.0285 - val_mean: 0.0285\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.1886 - mean: 0.1886 - val_loss: 0.4226 - val_mean: 0.4227\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0600 - mean: 0.0600 - val_loss: 0.0370 - val_mean: 0.0370\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0330 - mean: 0.0330 - val_loss: 0.0313 - val_mean: 0.0313\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0292 - mean: 0.0292 - val_loss: 0.0287 - val_mean: 0.0287\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0270 - mean: 0.0270 - val_loss: 0.0269 - val_mean: 0.0269\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0256 - mean: 0.0256 - val_loss: 0.0257 - val_mean: 0.0257\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0246 - mean: 0.0246 - val_loss: 0.0248 - val_mean: 0.0248\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0238 - mean: 0.0238 - val_loss: 0.0242 - val_mean: 0.0242\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0232 - mean: 0.0232 - val_loss: 0.0237 - val_mean: 0.0237\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0227 - mean: 0.0227 - val_loss: 0.0232 - val_mean: 0.0232\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0223 - mean: 0.0223 - val_loss: 0.0229 - val_mean: 0.0229\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0219 - mean: 0.0219 - val_loss: 0.0226 - val_mean: 0.0226\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0216 - mean: 0.0216 - val_loss: 0.0224 - val_mean: 0.0224\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0213 - mean: 0.0213 - val_loss: 0.0221 - val_mean: 0.0221\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0210 - mean: 0.0210 - val_loss: 0.0219 - val_mean: 0.0219\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0207 - mean: 0.0207 - val_loss: 0.0217 - val_mean: 0.0217\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0203 - mean: 0.0203 - val_loss: 0.0215 - val_mean: 0.0215\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0200 - mean: 0.0200 - val_loss: 0.0213 - val_mean: 0.0213\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0196 - mean: 0.0196 - val_loss: 0.0211 - val_mean: 0.0211\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0192 - mean: 0.0192 - val_loss: 0.0208 - val_mean: 0.0208\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0187 - mean: 0.0187 - val_loss: 0.0205 - val_mean: 0.0205\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0182 - mean: 0.0182 - val_loss: 0.0202 - val_mean: 0.0202\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0176 - mean: 0.0176 - val_loss: 0.0198 - val_mean: 0.0198\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0170 - mean: 0.0170 - val_loss: 0.0195 - val_mean: 0.0195\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0164 - mean: 0.0164 - val_loss: 0.0192 - val_mean: 0.0192\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0159 - mean: 0.0159 - val_loss: 0.0189 - val_mean: 0.0189\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0153 - mean: 0.0153 - val_loss: 0.0187 - val_mean: 0.0187\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0147 - mean: 0.0147 - val_loss: 0.0184 - val_mean: 0.0184\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0142 - mean: 0.0142 - val_loss: 0.0183 - val_mean: 0.0183\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0137 - mean: 0.0137 - val_loss: 0.0182 - val_mean: 0.0182\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0132 - mean: 0.0132 - val_loss: 0.0181 - val_mean: 0.0181\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0127 - mean: 0.0127 - val_loss: 0.0181 - val_mean: 0.0181\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0122 - mean: 0.0122 - val_loss: 0.0181 - val_mean: 0.0181\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0117 - mean: 0.0117 - val_loss: 0.0181 - val_mean: 0.0181\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0113 - mean: 0.0113 - val_loss: 0.0182 - val_mean: 0.0182\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0109 - mean: 0.0109 - val_loss: 0.0183 - val_mean: 0.0183\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0104 - mean: 0.0104 - val_loss: 0.0183 - val_mean: 0.0183\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0100 - mean: 0.0100 - val_loss: 0.0185 - val_mean: 0.0185\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0096 - mean: 0.0096 - val_loss: 0.0185 - val_mean: 0.0185\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0093 - mean: 0.0093 - val_loss: 0.0187 - val_mean: 0.0187\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0187 - mean: 0.0187\n","Final MSE on test data: 0.018686100840568542\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 14s 17ms/step - loss: 10.9218 - mean: 10.9207 - val_loss: 0.3900 - val_mean: 0.3899\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2027 - mean: 0.2027 - val_loss: 0.1119 - val_mean: 0.1119\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0808 - mean: 0.0808 - val_loss: 0.0622 - val_mean: 0.0622\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0520 - mean: 0.0520 - val_loss: 0.0452 - val_mean: 0.0452\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0410 - mean: 0.0410 - val_loss: 0.0380 - val_mean: 0.0380\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0351 - mean: 0.0351 - val_loss: 0.0333 - val_mean: 0.0333\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0313 - mean: 0.0313 - val_loss: 0.0303 - val_mean: 0.0303\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0289 - mean: 0.0289 - val_loss: 0.0284 - val_mean: 0.0284\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0270 - mean: 0.0270 - val_loss: 0.0268 - val_mean: 0.0268\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0440 - mean: 0.0440 - val_loss: 0.0272 - val_mean: 0.0272\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0252 - mean: 0.0252 - val_loss: 0.0246 - val_mean: 0.0246\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0235 - mean: 0.0235 - val_loss: 0.0234 - val_mean: 0.0234\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0226 - mean: 0.0226 - val_loss: 0.0227 - val_mean: 0.0227\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0219 - mean: 0.0219 - val_loss: 0.0221 - val_mean: 0.0221\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0214 - mean: 0.0214 - val_loss: 0.0217 - val_mean: 0.0217\n","Epoch 16/50\n","586/586 [==============================] - 9s 16ms/step - loss: 0.0209 - mean: 0.0209 - val_loss: 0.0213 - val_mean: 0.0213\n","Epoch 17/50\n","586/586 [==============================] - 9s 16ms/step - loss: 0.0205 - mean: 0.0205 - val_loss: 0.0209 - val_mean: 0.0209\n","Epoch 18/50\n","586/586 [==============================] - 9s 16ms/step - loss: 0.0201 - mean: 0.0201 - val_loss: 0.0206 - val_mean: 0.0206\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0197 - mean: 0.0197 - val_loss: 0.0203 - val_mean: 0.0203\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0194 - mean: 0.0194 - val_loss: 0.0200 - val_mean: 0.0200\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0190 - mean: 0.0190 - val_loss: 0.0196 - val_mean: 0.0196\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0186 - mean: 0.0186 - val_loss: 0.0194 - val_mean: 0.0194\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0182 - mean: 0.0182 - val_loss: 0.0191 - val_mean: 0.0191\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0178 - mean: 0.0178 - val_loss: 0.0188 - val_mean: 0.0188\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0174 - mean: 0.0174 - val_loss: 0.0185 - val_mean: 0.0185\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0169 - mean: 0.0169 - val_loss: 0.0181 - val_mean: 0.0181\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0164 - mean: 0.0164 - val_loss: 0.0178 - val_mean: 0.0178\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0158 - mean: 0.0158 - val_loss: 0.0174 - val_mean: 0.0174\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0152 - mean: 0.0152 - val_loss: 0.0170 - val_mean: 0.0170\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0145 - mean: 0.0145 - val_loss: 0.0166 - val_mean: 0.0166\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0139 - mean: 0.0139 - val_loss: 0.0162 - val_mean: 0.0162\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0132 - mean: 0.0132 - val_loss: 0.0158 - val_mean: 0.0158\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0126 - mean: 0.0126 - val_loss: 0.0154 - val_mean: 0.0154\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0121 - mean: 0.0121 - val_loss: 0.0151 - val_mean: 0.0151\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0115 - mean: 0.0115 - val_loss: 0.0149 - val_mean: 0.0149\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0110 - mean: 0.0110 - val_loss: 0.0147 - val_mean: 0.0147\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0106 - mean: 0.0106 - val_loss: 0.0146 - val_mean: 0.0146\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0101 - mean: 0.0101 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0097 - mean: 0.0097 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0093 - mean: 0.0093 - val_loss: 0.0143 - val_mean: 0.0143\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0089 - mean: 0.0089 - val_loss: 0.0143 - val_mean: 0.0143\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0085 - mean: 0.0085 - val_loss: 0.0143 - val_mean: 0.0143\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0081 - mean: 0.0081 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0078 - mean: 0.0078 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0074 - mean: 0.0074 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0071 - mean: 0.0071 - val_loss: 0.0145 - val_mean: 0.0145\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0068 - mean: 0.0068 - val_loss: 0.0146 - val_mean: 0.0146\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0065 - mean: 0.0065 - val_loss: 0.0147 - val_mean: 0.0147\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0063 - mean: 0.0063 - val_loss: 0.0147 - val_mean: 0.0147\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0061 - mean: 0.0061 - val_loss: 0.0148 - val_mean: 0.0148\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0148 - mean: 0.0148\n","Final MSE on test data: 0.014779039658606052\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 13s 17ms/step - loss: 12.3775 - mean: 12.3763 - val_loss: 0.4748 - val_mean: 0.4748\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2313 - mean: 0.2313 - val_loss: 0.1180 - val_mean: 0.1180\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0841 - mean: 0.0841 - val_loss: 0.0635 - val_mean: 0.0635\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0522 - mean: 0.0522 - val_loss: 0.0449 - val_mean: 0.0449\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0400 - mean: 0.0400 - val_loss: 0.0373 - val_mean: 0.0373\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0485 - mean: 0.0485 - val_loss: 0.0331 - val_mean: 0.0331\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0298 - mean: 0.0298 - val_loss: 0.0286 - val_mean: 0.0286\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0266 - mean: 0.0266 - val_loss: 0.0260 - val_mean: 0.0260\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0245 - mean: 0.0245 - val_loss: 0.0243 - val_mean: 0.0243\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0230 - mean: 0.0230 - val_loss: 0.0228 - val_mean: 0.0228\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0217 - mean: 0.0217 - val_loss: 0.0218 - val_mean: 0.0218\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0207 - mean: 0.0207 - val_loss: 0.0207 - val_mean: 0.0207\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0198 - mean: 0.0198 - val_loss: 0.0200 - val_mean: 0.0200\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2172 - mean: 0.2172 - val_loss: 0.0310 - val_mean: 0.0310\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0254 - mean: 0.0254 - val_loss: 0.0232 - val_mean: 0.0232\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0214 - mean: 0.0214 - val_loss: 0.0210 - val_mean: 0.0210\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0198 - mean: 0.0198 - val_loss: 0.0198 - val_mean: 0.0198\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0188 - mean: 0.0188 - val_loss: 0.0190 - val_mean: 0.0190\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0181 - mean: 0.0181 - val_loss: 0.0183 - val_mean: 0.0183\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0175 - mean: 0.0175 - val_loss: 0.0178 - val_mean: 0.0178\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0170 - mean: 0.0170 - val_loss: 0.0174 - val_mean: 0.0174\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0166 - mean: 0.0166 - val_loss: 0.0170 - val_mean: 0.0170\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0161 - mean: 0.0161 - val_loss: 0.0166 - val_mean: 0.0166\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0157 - mean: 0.0157 - val_loss: 0.0162 - val_mean: 0.0162\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0153 - mean: 0.0153 - val_loss: 0.0158 - val_mean: 0.0158\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0149 - mean: 0.0149 - val_loss: 0.0154 - val_mean: 0.0154\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0144 - mean: 0.0144 - val_loss: 0.0150 - val_mean: 0.0150\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0139 - mean: 0.0139 - val_loss: 0.0147 - val_mean: 0.0147\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0134 - mean: 0.0134 - val_loss: 0.0142 - val_mean: 0.0142\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0129 - mean: 0.0129 - val_loss: 0.0138 - val_mean: 0.0138\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0123 - mean: 0.0123 - val_loss: 0.0133 - val_mean: 0.0133\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0116 - mean: 0.0116 - val_loss: 0.0128 - val_mean: 0.0128\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0110 - mean: 0.0110 - val_loss: 0.0123 - val_mean: 0.0123\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0103 - mean: 0.0103 - val_loss: 0.0117 - val_mean: 0.0117\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0096 - mean: 0.0096 - val_loss: 0.0112 - val_mean: 0.0112\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0090 - mean: 0.0090 - val_loss: 0.0108 - val_mean: 0.0108\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0085 - mean: 0.0085 - val_loss: 0.0104 - val_mean: 0.0104\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0080 - mean: 0.0080 - val_loss: 0.0100 - val_mean: 0.0100\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0075 - mean: 0.0075 - val_loss: 0.0097 - val_mean: 0.0097\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0071 - mean: 0.0071 - val_loss: 0.0094 - val_mean: 0.0094\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0067 - mean: 0.0067 - val_loss: 0.0092 - val_mean: 0.0092\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0063 - mean: 0.0063 - val_loss: 0.0090 - val_mean: 0.0090\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0060 - mean: 0.0060 - val_loss: 0.0089 - val_mean: 0.0088\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0057 - mean: 0.0057 - val_loss: 0.0087 - val_mean: 0.0087\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0054 - mean: 0.0054 - val_loss: 0.0086 - val_mean: 0.0086\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0051 - mean: 0.0051 - val_loss: 0.0085 - val_mean: 0.0085\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0048 - mean: 0.0048 - val_loss: 0.0085 - val_mean: 0.0085\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0045 - mean: 0.0045 - val_loss: 0.0084 - val_mean: 0.0084\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0043 - mean: 0.0043 - val_loss: 0.0084 - val_mean: 0.0084\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0041 - mean: 0.0041 - val_loss: 0.0084 - val_mean: 0.0084\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0084 - mean: 0.0084\n","Final MSE on test data: 0.008388441056013107\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 14s 17ms/step - loss: 7.3793 - mean: 7.3785 - val_loss: 0.2335 - val_mean: 0.2335\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.1206 - mean: 0.1206 - val_loss: 0.0675 - val_mean: 0.0675\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0519 - mean: 0.0519 - val_loss: 0.0420 - val_mean: 0.0420\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0364 - mean: 0.0364 - val_loss: 0.0327 - val_mean: 0.0327\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0296 - mean: 0.0296 - val_loss: 0.0280 - val_mean: 0.0280\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0429 - mean: 0.0429 - val_loss: 0.0321 - val_mean: 0.0322\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0262 - mean: 0.0262 - val_loss: 0.0240 - val_mean: 0.0240\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0222 - mean: 0.0222 - val_loss: 0.0216 - val_mean: 0.0216\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0204 - mean: 0.0204 - val_loss: 0.0202 - val_mean: 0.0202\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0192 - mean: 0.0192 - val_loss: 0.0192 - val_mean: 0.0192\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0183 - mean: 0.0183 - val_loss: 0.0183 - val_mean: 0.0183\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0175 - mean: 0.0175 - val_loss: 0.0176 - val_mean: 0.0176\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0168 - mean: 0.0168 - val_loss: 0.0170 - val_mean: 0.0170\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0162 - mean: 0.0162 - val_loss: 0.0164 - val_mean: 0.0164\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0157 - mean: 0.0157 - val_loss: 0.0159 - val_mean: 0.0159\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0151 - mean: 0.0151 - val_loss: 0.0154 - val_mean: 0.0154\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0146 - mean: 0.0146 - val_loss: 0.0149 - val_mean: 0.0149\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0141 - mean: 0.0141 - val_loss: 0.0145 - val_mean: 0.0145\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0135 - mean: 0.0135 - val_loss: 0.0140 - val_mean: 0.0140\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0130 - mean: 0.0130 - val_loss: 0.0134 - val_mean: 0.0134\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0123 - mean: 0.0123 - val_loss: 0.0129 - val_mean: 0.0129\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0117 - mean: 0.0117 - val_loss: 0.0123 - val_mean: 0.0123\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0110 - mean: 0.0110 - val_loss: 0.0117 - val_mean: 0.0117\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0103 - mean: 0.0103 - val_loss: 0.0110 - val_mean: 0.0110\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0095 - mean: 0.0095 - val_loss: 0.0104 - val_mean: 0.0104\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0087 - mean: 0.0087 - val_loss: 0.0098 - val_mean: 0.0098\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0080 - mean: 0.0080 - val_loss: 0.0091 - val_mean: 0.0091\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0073 - mean: 0.0073 - val_loss: 0.0086 - val_mean: 0.0086\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0067 - mean: 0.0067 - val_loss: 0.0080 - val_mean: 0.0080\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0062 - mean: 0.0062 - val_loss: 0.0076 - val_mean: 0.0076\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0057 - mean: 0.0057 - val_loss: 0.0072 - val_mean: 0.0072\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0053 - mean: 0.0053 - val_loss: 0.0069 - val_mean: 0.0069\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0050 - mean: 0.0050 - val_loss: 0.0066 - val_mean: 0.0066\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0046 - mean: 0.0046 - val_loss: 0.0063 - val_mean: 0.0063\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0044 - mean: 0.0044 - val_loss: 0.0061 - val_mean: 0.0061\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0041 - mean: 0.0041 - val_loss: 0.0059 - val_mean: 0.0059\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0038 - mean: 0.0038 - val_loss: 0.0057 - val_mean: 0.0057\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0036 - mean: 0.0036 - val_loss: 0.0056 - val_mean: 0.0056\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0034 - mean: 0.0034 - val_loss: 0.0055 - val_mean: 0.0055\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0032 - mean: 0.0032 - val_loss: 0.0054 - val_mean: 0.0054\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0031 - mean: 0.0031 - val_loss: 0.0053 - val_mean: 0.0053\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0029 - mean: 0.0029 - val_loss: 0.0052 - val_mean: 0.0052\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0028 - mean: 0.0028 - val_loss: 0.0051 - val_mean: 0.0051\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0026 - mean: 0.0026 - val_loss: 0.0051 - val_mean: 0.0051\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0025 - mean: 0.0025 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0024 - mean: 0.0024 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0023 - mean: 0.0023 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0022 - mean: 0.0022 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0021 - mean: 0.0021 - val_loss: 0.0049 - val_mean: 0.0049\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0020 - mean: 0.0020 - val_loss: 0.0049 - val_mean: 0.0049\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0049 - mean: 0.0049\n","Final MSE on test data: 0.004931195639073849\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 14s 17ms/step - loss: 15.2200 - mean: 15.2184 - val_loss: 0.4757 - val_mean: 0.4757\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2285 - mean: 0.2285 - val_loss: 0.1159 - val_mean: 0.1159\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0824 - mean: 0.0824 - val_loss: 0.0624 - val_mean: 0.0624\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0512 - mean: 0.0512 - val_loss: 0.0439 - val_mean: 0.0439\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0388 - mean: 0.0388 - val_loss: 0.0352 - val_mean: 0.0353\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0466 - mean: 0.0466 - val_loss: 0.0392 - val_mean: 0.0392\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0301 - mean: 0.0301 - val_loss: 0.0277 - val_mean: 0.0277\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0255 - mean: 0.0255 - val_loss: 0.0248 - val_mean: 0.0248\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0232 - mean: 0.0232 - val_loss: 0.0229 - val_mean: 0.0229\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0216 - mean: 0.0216 - val_loss: 0.0215 - val_mean: 0.0215\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0203 - mean: 0.0203 - val_loss: 0.0203 - val_mean: 0.0203\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0193 - mean: 0.0193 - val_loss: 0.0194 - val_mean: 0.0194\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0184 - mean: 0.0184 - val_loss: 0.0186 - val_mean: 0.0186\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0177 - mean: 0.0177 - val_loss: 0.0179 - val_mean: 0.0179\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0170 - mean: 0.0170 - val_loss: 0.0173 - val_mean: 0.0173\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0164 - mean: 0.0164 - val_loss: 0.0166 - val_mean: 0.0166\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2489 - mean: 0.2489 - val_loss: 0.0276 - val_mean: 0.0276\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0226 - mean: 0.0226 - val_loss: 0.0207 - val_mean: 0.0207\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0189 - mean: 0.0189 - val_loss: 0.0187 - val_mean: 0.0187\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0174 - mean: 0.0174 - val_loss: 0.0175 - val_mean: 0.0175\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0165 - mean: 0.0165 - val_loss: 0.0168 - val_mean: 0.0168\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0159 - mean: 0.0159 - val_loss: 0.0162 - val_mean: 0.0162\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0154 - mean: 0.0154 - val_loss: 0.0158 - val_mean: 0.0158\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0150 - mean: 0.0150 - val_loss: 0.0154 - val_mean: 0.0154\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0146 - mean: 0.0146 - val_loss: 0.0150 - val_mean: 0.0150\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0142 - mean: 0.0142 - val_loss: 0.0146 - val_mean: 0.0146\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0138 - mean: 0.0138 - val_loss: 0.0143 - val_mean: 0.0143\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0134 - mean: 0.0134 - val_loss: 0.0139 - val_mean: 0.0139\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0130 - mean: 0.0130 - val_loss: 0.0135 - val_mean: 0.0135\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0125 - mean: 0.0125 - val_loss: 0.0131 - val_mean: 0.0131\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0120 - mean: 0.0120 - val_loss: 0.0126 - val_mean: 0.0126\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0114 - mean: 0.0114 - val_loss: 0.0120 - val_mean: 0.0120\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0108 - mean: 0.0108 - val_loss: 0.0115 - val_mean: 0.0115\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0101 - mean: 0.0101 - val_loss: 0.0109 - val_mean: 0.0109\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0094 - mean: 0.0094 - val_loss: 0.0102 - val_mean: 0.0102\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0086 - mean: 0.0086 - val_loss: 0.0096 - val_mean: 0.0096\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0079 - mean: 0.0079 - val_loss: 0.0089 - val_mean: 0.0089\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0073 - mean: 0.0073 - val_loss: 0.0084 - val_mean: 0.0084\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0066 - mean: 0.0066 - val_loss: 0.0078 - val_mean: 0.0078\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0061 - mean: 0.0061 - val_loss: 0.0073 - val_mean: 0.0073\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0056 - mean: 0.0056 - val_loss: 0.0069 - val_mean: 0.0069\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0052 - mean: 0.0052 - val_loss: 0.0065 - val_mean: 0.0065\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0048 - mean: 0.0048 - val_loss: 0.0061 - val_mean: 0.0061\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0044 - mean: 0.0044 - val_loss: 0.0059 - val_mean: 0.0059\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0041 - mean: 0.0041 - val_loss: 0.0056 - val_mean: 0.0056\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0038 - mean: 0.0038 - val_loss: 0.0054 - val_mean: 0.0054\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0036 - mean: 0.0036 - val_loss: 0.0051 - val_mean: 0.0051\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0033 - mean: 0.0033 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0031 - mean: 0.0031 - val_loss: 0.0048 - val_mean: 0.0048\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0029 - mean: 0.0029 - val_loss: 0.0047 - val_mean: 0.0047\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0047 - mean: 0.0047\n","Final MSE on test data: 0.004690547939389944\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 14s 17ms/step - loss: 8.8222 - mean: 8.8213 - val_loss: 0.2507 - val_mean: 0.2507\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.1276 - mean: 0.1276 - val_loss: 0.0722 - val_mean: 0.0722\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0548 - mean: 0.0548 - val_loss: 0.0444 - val_mean: 0.0444\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0384 - mean: 0.0384 - val_loss: 0.0345 - val_mean: 0.0345\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0310 - mean: 0.0310 - val_loss: 0.0292 - val_mean: 0.0292\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0382 - mean: 0.0382 - val_loss: 0.0279 - val_mean: 0.0279\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0249 - mean: 0.0249 - val_loss: 0.0238 - val_mean: 0.0238\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0222 - mean: 0.0222 - val_loss: 0.0218 - val_mean: 0.0218\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0206 - mean: 0.0206 - val_loss: 0.0204 - val_mean: 0.0204\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0194 - mean: 0.0194 - val_loss: 0.0193 - val_mean: 0.0193\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0184 - mean: 0.0184 - val_loss: 0.0185 - val_mean: 0.0185\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0176 - mean: 0.0176 - val_loss: 0.0177 - val_mean: 0.0177\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0169 - mean: 0.0169 - val_loss: 0.0170 - val_mean: 0.0170\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0163 - mean: 0.0163 - val_loss: 0.0165 - val_mean: 0.0165\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0157 - mean: 0.0157 - val_loss: 0.0159 - val_mean: 0.0159\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0152 - mean: 0.0152 - val_loss: 0.0154 - val_mean: 0.0154\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.2156 - mean: 0.2156 - val_loss: 0.0240 - val_mean: 0.0240\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0202 - mean: 0.0202 - val_loss: 0.0191 - val_mean: 0.0191\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0174 - mean: 0.0174 - val_loss: 0.0174 - val_mean: 0.0174\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0162 - mean: 0.0162 - val_loss: 0.0164 - val_mean: 0.0164\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0154 - mean: 0.0154 - val_loss: 0.0157 - val_mean: 0.0157\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0148 - mean: 0.0148 - val_loss: 0.0152 - val_mean: 0.0152\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0144 - mean: 0.0144 - val_loss: 0.0148 - val_mean: 0.0148\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0140 - mean: 0.0140 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0136 - mean: 0.0136 - val_loss: 0.0141 - val_mean: 0.0141\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0133 - mean: 0.0133 - val_loss: 0.0138 - val_mean: 0.0138\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0129 - mean: 0.0129 - val_loss: 0.0134 - val_mean: 0.0134\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0126 - mean: 0.0126 - val_loss: 0.0131 - val_mean: 0.0131\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0122 - mean: 0.0122 - val_loss: 0.0127 - val_mean: 0.0127\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0118 - mean: 0.0118 - val_loss: 0.0124 - val_mean: 0.0124\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0113 - mean: 0.0113 - val_loss: 0.0119 - val_mean: 0.0119\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0108 - mean: 0.0108 - val_loss: 0.0115 - val_mean: 0.0115\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0102 - mean: 0.0102 - val_loss: 0.0110 - val_mean: 0.0110\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0096 - mean: 0.0096 - val_loss: 0.0104 - val_mean: 0.0104\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0090 - mean: 0.0090 - val_loss: 0.0098 - val_mean: 0.0098\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0083 - mean: 0.0083 - val_loss: 0.0092 - val_mean: 0.0092\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0076 - mean: 0.0076 - val_loss: 0.0086 - val_mean: 0.0086\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0070 - mean: 0.0070 - val_loss: 0.0081 - val_mean: 0.0081\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0064 - mean: 0.0064 - val_loss: 0.0076 - val_mean: 0.0076\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0058 - mean: 0.0058 - val_loss: 0.0070 - val_mean: 0.0070\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0054 - mean: 0.0054 - val_loss: 0.0066 - val_mean: 0.0066\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0049 - mean: 0.0049 - val_loss: 0.0062 - val_mean: 0.0062\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0045 - mean: 0.0045 - val_loss: 0.0058 - val_mean: 0.0058\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0042 - mean: 0.0042 - val_loss: 0.0055 - val_mean: 0.0055\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0039 - mean: 0.0039 - val_loss: 0.0052 - val_mean: 0.0052\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0036 - mean: 0.0036 - val_loss: 0.0050 - val_mean: 0.0050\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0033 - mean: 0.0033 - val_loss: 0.0048 - val_mean: 0.0048\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0031 - mean: 0.0031 - val_loss: 0.0046 - val_mean: 0.0046\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0029 - mean: 0.0029 - val_loss: 0.0044 - val_mean: 0.0044\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0027 - mean: 0.0027 - val_loss: 0.0043 - val_mean: 0.0043\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0043 - mean: 0.0043\n","Final MSE on test data: 0.004279817920178175\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n","Epoch 1/50\n","586/586 [==============================] - 14s 17ms/step - loss: 7.7744 - mean: 7.7736 - val_loss: 0.2386 - val_mean: 0.2387\n","Epoch 2/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.1190 - mean: 0.1190 - val_loss: 0.0677 - val_mean: 0.0677\n","Epoch 3/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0510 - mean: 0.0510 - val_loss: 0.0421 - val_mean: 0.0421\n","Epoch 4/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0363 - mean: 0.0363 - val_loss: 0.0330 - val_mean: 0.0330\n","Epoch 5/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0296 - mean: 0.0296 - val_loss: 0.0276 - val_mean: 0.0276\n","Epoch 6/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0346 - mean: 0.0346 - val_loss: 0.0252 - val_mean: 0.0252\n","Epoch 7/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0232 - mean: 0.0232 - val_loss: 0.0226 - val_mean: 0.0226\n","Epoch 8/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0213 - mean: 0.0213 - val_loss: 0.0210 - val_mean: 0.0210\n","Epoch 9/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0199 - mean: 0.0199 - val_loss: 0.0197 - val_mean: 0.0197\n","Epoch 10/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0188 - mean: 0.0188 - val_loss: 0.0187 - val_mean: 0.0187\n","Epoch 11/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0179 - mean: 0.0179 - val_loss: 0.0180 - val_mean: 0.0180\n","Epoch 12/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0171 - mean: 0.0171 - val_loss: 0.0173 - val_mean: 0.0173\n","Epoch 13/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0164 - mean: 0.0164 - val_loss: 0.0166 - val_mean: 0.0166\n","Epoch 14/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0158 - mean: 0.0158 - val_loss: 0.0160 - val_mean: 0.0160\n","Epoch 15/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0152 - mean: 0.0152 - val_loss: 0.0155 - val_mean: 0.0155\n","Epoch 16/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0147 - mean: 0.0147 - val_loss: 0.0149 - val_mean: 0.0149\n","Epoch 17/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0141 - mean: 0.0141 - val_loss: 0.0144 - val_mean: 0.0144\n","Epoch 18/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0136 - mean: 0.0136 - val_loss: 0.0139 - val_mean: 0.0139\n","Epoch 19/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0130 - mean: 0.0130 - val_loss: 0.0133 - val_mean: 0.0133\n","Epoch 20/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0123 - mean: 0.0123 - val_loss: 0.0127 - val_mean: 0.0127\n","Epoch 21/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0117 - mean: 0.0117 - val_loss: 0.0121 - val_mean: 0.0121\n","Epoch 22/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0110 - mean: 0.0110 - val_loss: 0.0115 - val_mean: 0.0115\n","Epoch 23/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0102 - mean: 0.0102 - val_loss: 0.0108 - val_mean: 0.0108\n","Epoch 24/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0094 - mean: 0.0094 - val_loss: 0.0101 - val_mean: 0.0101\n","Epoch 25/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0086 - mean: 0.0086 - val_loss: 0.0094 - val_mean: 0.0094\n","Epoch 26/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0078 - mean: 0.0078 - val_loss: 0.0087 - val_mean: 0.0087\n","Epoch 27/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0071 - mean: 0.0071 - val_loss: 0.0080 - val_mean: 0.0080\n","Epoch 28/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0064 - mean: 0.0064 - val_loss: 0.0075 - val_mean: 0.0075\n","Epoch 29/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0058 - mean: 0.0058 - val_loss: 0.0069 - val_mean: 0.0069\n","Epoch 30/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0053 - mean: 0.0053 - val_loss: 0.0064 - val_mean: 0.0064\n","Epoch 31/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0048 - mean: 0.0048 - val_loss: 0.0060 - val_mean: 0.0060\n","Epoch 32/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0044 - mean: 0.0044 - val_loss: 0.0057 - val_mean: 0.0057\n","Epoch 33/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0041 - mean: 0.0041 - val_loss: 0.0053 - val_mean: 0.0053\n","Epoch 34/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0038 - mean: 0.0038 - val_loss: 0.0051 - val_mean: 0.0051\n","Epoch 35/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0035 - mean: 0.0035 - val_loss: 0.0048 - val_mean: 0.0048\n","Epoch 36/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0033 - mean: 0.0033 - val_loss: 0.0046 - val_mean: 0.0046\n","Epoch 37/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0030 - mean: 0.0030 - val_loss: 0.0044 - val_mean: 0.0044\n","Epoch 38/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0028 - mean: 0.0028 - val_loss: 0.0042 - val_mean: 0.0042\n","Epoch 39/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0027 - mean: 0.0027 - val_loss: 0.0040 - val_mean: 0.0040\n","Epoch 40/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0025 - mean: 0.0025 - val_loss: 0.0039 - val_mean: 0.0039\n","Epoch 41/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0023 - mean: 0.0023 - val_loss: 0.0038 - val_mean: 0.0038\n","Epoch 42/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0022 - mean: 0.0022 - val_loss: 0.0037 - val_mean: 0.0037\n","Epoch 43/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0021 - mean: 0.0021 - val_loss: 0.0036 - val_mean: 0.0036\n","Epoch 44/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0019 - mean: 0.0019 - val_loss: 0.0035 - val_mean: 0.0035\n","Epoch 45/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0018 - mean: 0.0018 - val_loss: 0.0034 - val_mean: 0.0034\n","Epoch 46/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0017 - mean: 0.0017 - val_loss: 0.0034 - val_mean: 0.0034\n","Epoch 47/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0016 - mean: 0.0016 - val_loss: 0.0033 - val_mean: 0.0033\n","Epoch 48/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0015 - mean: 0.0015 - val_loss: 0.0033 - val_mean: 0.0033\n","Epoch 49/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0014 - mean: 0.0014 - val_loss: 0.0032 - val_mean: 0.0032\n","Epoch 50/50\n","586/586 [==============================] - 9s 15ms/step - loss: 0.0013 - mean: 0.0013 - val_loss: 0.0032 - val_mean: 0.0032\n","782/782 [==============================] - 3s 4ms/step - loss: 0.0032 - mean: 0.0032\n","Final MSE on test data: 0.0031665139831602573\n","writing to files\n","782/782 [==============================] - 3s 3ms/step\n","file writing done\n"]}],"source":["for snrdb in snr_dB:\n","    # data set has stacked A's and y's if dual_bs\n","    if dual_bs:\n","      # create sensing matrix A - will stack so energy of col of A = 1/2\n","      A = np.sqrt(1/(4*n))*np.random.randn(n, N) + 1j*np.sqrt(1/(4*n))*np.random.randn(n, N)  \n","    else:\n","      A = np.sqrt(1/(2*n))*np.random.randn(n, N) + 1j*np.sqrt(1/(2*n))*np.random.randn(n, N)  \n","      \n","    if fading:\n","      h1 = np.sqrt(1/2)*np.random.randn(N) + 1j*np.sqrt(1/2)*np.random.randn(N) \n","      h1 = np.diag(h1) \n","      \n","      A1 = A @ h1\n","\n","      if dual_bs:\n","        # now stack and make one A\n","        h2 = np.sqrt(1/2)*np.random.randn(N) + 1j*np.sqrt(1/2)*np.random.randn(N) \n","        h2 = np.diag(h2) \n","        \n","        A2 = A @ h2\n","        A = np.vstack((A1, A2))\n","      else:\n","        A = A1\n","        \n","    elif dual_bs: # no fading, 2 BS\n","      # A is just stacked with itself if no fading -- will get overwritten if fading\n","      A = np.vstack((A,A))\n","      \n","\n","\n","    # creates the appropiate sigma\n","    if dual_bs:\n","      snr = (10**(snrdb/10))*2*n\n","    else:\n","      snr = (10**(snrdb/10))*n\n","    sigma = 1/(np.sqrt(snr))\n","\n","\n","    # noise vectors\n","    w1 = []\n","    for i in range(total_num_samples):\n","      w1.append(np.sqrt((sigma**2)/2) * np.random.randn(n) + 1j*np.sqrt((sigma**2)/2) * np.random.randn(n))\n","    w1 = np.array(w1)\n","\n","    if dual_bs:\n","      w2 = []\n","      for i in range(total_num_samples):\n","        w2.append(np.sqrt((sigma**2)/2) * np.random.randn(n) + 1j*np.sqrt((sigma**2)/2) * np.random.randn(n))\n","      w2 = np.array(w2)\n","      # stack the noise\n","      w = np.hstack((w1,w2))\n","    else:\n","      w = w1\n","\n","    # inital alpha value\n","    init_alpha = 0.3\n","\n","    # creating initial values for trainable parameters\n","    _, Lambda, _ = np.linalg.svd(A)\n","    L = np.max(Lambda) + 1\n","\n","    init_Q = np.eye(N) - (A.T @ A) / L\n","    init_W = A / L\n","\n","    # create data\n","    x = np.zeros((total_num_samples, N))\n","    idx_nonzero = rng.permuted(np.tile(np.arange(N), (total_num_samples, 1)), axis=1)\n","    for i in range(k):\n","      x[np.arange(total_num_samples), idx_nonzero[:, i].flatten()] = 1\n","\n","      \n","    if noise:\n","      y = x @ A.T + w  # adding noise here\n","    else:\n","      y = x @ A.T\n","    y = np.hstack((np.zeros((total_num_samples, N)), y))\n","\n","    # Divide data into training, validation, testing sets\n","    train_data = y[0:num_train_samples, :]\n","    train_labels = x[0:num_train_samples, :]\n","    valid_data = y[num_train_samples:num_train_samples+num_valid_samples, :]\n","    valid_labels = x[num_train_samples:num_train_samples+num_valid_samples, :]\n","    test_data = y[num_train_samples+num_valid_samples:, :]\n","    test_labels = x[num_train_samples+num_valid_samples:, :]\n"," ########################################################\n","    lista_mse_cmplx = []\n","    learn_rate = .0001\n","    op = keras.optimizers.Adam(learning_rate=learn_rate)\n","\n","    #epoch_nums = [20,20,20,20,20,20,22,25,28,31,34,40,43,46,50]\n","    epochs = 50\n","\n","    if dual_bs:\n","    # create initial input -- shape is now (2n + N)\n","        input = keras.layers.Input(shape=(2*n+N,), name='y_inputs',dtype='complex64')\n","    else:\n","        input = keras.layers.Input(shape=(n+N,), name='y_inputs',dtype='complex64')\n","\n","    for idx_layer in range(num_layers):\n","      # calls the layer differently depending on what layer it is \n","        if idx_layer == 0:\n","          x_hidden = CRF_LISTALayer(A, L, init_alpha, init_Q, init_W) (input)\n","        elif idx_layer == num_layers - 1:  # output is populated on the last layer\n","          output = CRF_LISTALayer(A, L, init_alpha, init_Q, init_W) (x_hidden)\n","        else:\n","          x_hidden = CRF_LISTALayer(A, L, init_alpha, init_Q, init_W) (x_hidden)\n","\n","\n","    model = keras.Model(inputs=input, outputs=output, name='CRF_LISTAModel')\n","    # compile keras model\n","    custom_mse_loss = CustomMSELoss(N)\n","    custom_mse_metric = CustomMSEMetric(N)\n","    model.compile(optimizer=op, loss=custom_mse_loss, metrics=[custom_mse_metric])\n","\n","    # train model\n","    history = model.fit(train_data,\n","                train_labels,\n","                batch_size=128,\n","                epochs=epochs,\n","                validation_data=(valid_data, valid_labels))\n","\n","    results = model.evaluate(test_data, test_labels)\n","    lista_mse_cmplx.append(results[0])\n","    print(f'Final MSE on test data: {results[0]}')\n","\n","    ## find MD, FA ##\n","    print('writing to files')\n","    xHt = model.predict(test_data)\n","    np.save('1bs_snr'+str(snrdb)+'dB_mdfa_predictions.npy', xHt)\n","\n","    x = test_labels\n","    np.save('1bs_snr'+str(snrdb)+'dB_mdfa_labels.npy',x)\n","    print('file writing done')"]},{"cell_type":"markdown","metadata":{"id":"9deaQaltmcJA"},"source":["# MD, FA"]},{"cell_type":"code","source":["# this is how I saved the data to drive\n","!cp '1bs_snr20dB_mdfa_predictions.npy' /content/drive/MyDrive/Research\n","!cp '1bs_snr20dB_mdfa_labels.npy' /content/drive/MyDrive/Research"],"metadata":{"id":"Ax0gCko57T1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EaCShW10QqeN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680193420671,"user_tz":300,"elapsed":580185,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"}},"outputId":"2cff140d-f471-470e-9d49-de8ef4a79b05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold: 0.1\n","FA: [42.38759036144578, 31.32730923694779, 20.44831325301205, 11.220843373493976, 13.329036144578314, 12.978273092369477, 8.08433734939759]\n","MD: [8.61136546184739, 6.086827309236948, 2.42285140562249, 1.4061847389558233, 1.0512048192771084, 0.9588755020080322, 0.7850602409638554]\n","Threshold: 0.15000000000000002\n","FA: [28.21, 21.001767068273093, 12.781004016064257, 6.905261044176707, 7.434216867469879, 7.14570281124498, 4.790682730923694]\n","MD: [9.913775100401606, 7.0475502008032125, 2.9908032128514055, 1.6967469879518073, 1.3444979919678715, 1.2424899598393575, 0.9542570281124498]\n","Threshold: 0.20000000000000004\n","FA: [19.008353413654618, 14.276224899598393, 8.059839357429718, 4.1952208835341365, 4.073493975903615, 3.8368273092369476, 2.7790361445783134]\n","MD: [11.148152610441768, 7.989678714859438, 3.567670682730924, 1.9912048192771084, 1.6514859437751004, 1.5393975903614459, 1.1330522088353414]\n","Threshold: 0.25000000000000006\n","FA: [12.975662650602409, 9.823614457831326, 5.113734939759036, 2.506425702811245, 2.21285140562249, 2.0517670682730924, 1.5762650602409638]\n","MD: [12.32128514056225, 8.935140562248996, 4.170481927710844, 2.300722891566265, 1.970120481927711, 1.829879518072289, 1.3209236947791165]\n","Threshold: 0.30000000000000004\n","FA: [8.980522088353414, 6.854738955823293, 3.2803614457831327, 1.4916465863453816, 1.190160642570281, 1.0806827309236948, 0.8716867469879518]\n","MD: [13.459839357429718, 9.888273092369477, 4.778112449799197, 2.6109236947791166, 2.302168674698795, 2.107590361445783, 1.5136546184738955]\n","Threshold: 0.3500000000000001\n","FA: [6.29995983935743, 4.826827309236948, 2.1135742971887552, 0.8815261044176707, 0.6433734939759036, 0.5683935742971887, 0.4718875502008032]\n","MD: [14.569116465863454, 10.859477911646586, 5.433975903614458, 2.94004016064257, 2.6433734939759037, 2.3823293172690763, 1.7171084337349398]\n","Threshold: 0.40000000000000013\n","FA: [4.476746987951807, 3.4253413654618474, 1.3634136546184739, 0.5159839357429719, 0.3487550200803213, 0.3042168674698795, 0.2523293172690763]\n","MD: [15.68726907630522, 11.874497991967871, 6.140120481927711, 3.3104417670682733, 3.0344578313253012, 2.6889156626506026, 1.9396787148594377]\n","Threshold: 0.45000000000000007\n","FA: [3.1933333333333334, 2.4518072289156625, 0.890441767068273, 0.3032128514056225, 0.18248995983935742, 0.1602008032128514, 0.1379116465863454]\n","MD: [16.82289156626506, 12.943453815261044, 6.940602409638554, 3.726385542168675, 3.476104417670683, 3.038112449799197, 2.193975903614458]\n","Threshold: 0.5000000000000001\n","FA: [2.2996385542168674, 1.7639759036144578, 0.5766265060240964, 0.18152610441767067, 0.09738955823293173, 0.0823293172690763, 0.07409638554216867]\n","MD: [18.005020080321284, 14.09586345381526, 7.846144578313253, 4.223333333333334, 4.002048192771085, 3.4768273092369477, 2.497871485943775]\n","Threshold: 0.5500000000000002\n","FA: [1.643975903614458, 1.2685943775100401, 0.365140562248996, 0.11044176706827309, 0.05168674698795181, 0.04176706827309237, 0.03955823293172691]\n","MD: [19.229076305220882, 15.32305220883534, 8.883413654618474, 4.846184738955824, 4.653212851405622, 4.024859437751004, 2.882248995983936]\n","Threshold: 0.6000000000000002\n","FA: [1.1808032128514057, 0.913574297188755, 0.23032128514056224, 0.06493975903614457, 0.027590361445783133, 0.021927710843373496, 0.022690763052208834]\n","MD: [20.500160642570282, 16.637429718875502, 10.108755020080322, 5.650682730923695, 5.476064257028113, 4.737389558232931, 3.3807228915662653]\n","Threshold: 0.6500000000000001\n","FA: [0.8439759036144578, 0.6524497991967871, 0.1455020080321285, 0.03730923694779117, 0.01325301204819277, 0.009718875502008032, 0.012931726907630522]\n","MD: [21.80574297188755, 18.050722891566267, 11.541847389558233, 6.694658634538152, 6.533253012048193, 5.692208835341366, 4.076827309236948]\n","Threshold: 0.7000000000000002\n","FA: [0.6020883534136546, 0.4606024096385542, 0.0895582329317269, 0.020923694779116465, 0.006506024096385542, 0.004377510040160642, 0.00678714859437751]\n","MD: [23.170602409638555, 19.54975903614458, 13.19722891566265, 8.08289156626506, 7.924819277108433, 6.966987951807229, 5.068473895582329]\n","Threshold: 0.7500000000000002\n","FA: [0.42738955823293173, 0.3276305220883534, 0.056385542168674696, 0.012409638554216867, 0.00321285140562249, 0.002329317269076305, 0.0038554216867469878]\n","MD: [24.560803212851404, 21.139477911646587, 15.10152610441767, 9.899638554216867, 9.724779116465863, 8.681686746987952, 6.501526104417671]\n","Threshold: 0.8000000000000002\n","FA: [0.303855421686747, 0.2308835341365462, 0.034417670682730925, 0.007108433734939759, 0.0016465863453815261, 0.0011244979919678715, 0.0021285140562248995]\n","MD: [25.983574297188756, 22.79397590361446, 17.25433734939759, 12.202489959839358, 12.010602409638555, 10.945381526104418, 8.5785140562249]\n","Threshold: 0.8500000000000002\n","FA: [0.21670682730923696, 0.1614859437751004, 0.0214859437751004, 0.004297188755020081, 0.0008032128514056225, 0.00048192771084337347, 0.001325301204819277]\n","MD: [27.40020080321285, 24.50710843373494, 19.619036144578313, 15.044457831325301, 14.854738955823294, 13.805903614457831, 11.507228915662651]\n","Threshold: 0.9000000000000002\n","FA: [0.1527309236947791, 0.11305220883534137, 0.013493975903614458, 0.0023694779116465864, 0.0005220883534136547, 0.00020080321285140563, 0.0006827309236947791]\n","MD: [28.812530120481927, 26.242570281124497, 22.14714859437751, 18.408192771084337, 18.21566265060241, 17.261887550200804, 15.360722891566265]\n","Threshold: 0.9500000000000003\n","FA: [0.10771084337349397, 0.07963855421686747, 0.00783132530120482, 0.0013654618473895582, 0.00024096385542168674, 0.0001606425702811245, 0.00028112449799196787]\n","MD: [30.202650602409637, 27.96987951807229, 24.74285140562249, 22.114578313253013, 21.932369477911646, 21.165502008032128, 19.959317269076305]\n"]}],"source":["# determining threshold based on 24,900 test samples\n","test_to = 24900\n","thresholds = np.arange(.1,1,.05)\n","\n","for threshold in thresholds:\n","  avg_fa = []\n","  avg_md = []\n","  for snrdb in snr_dB:\n","    # load in data from the files\n","    # xHt = np.load('1bs_snr'+str(snrdb)+'dB_mdfa_predictions.npy')\n","    # labels = np.load('1bs_snr'+str(snrdb)+'dB_mdfa_labels.npy')\n","    name = \"/content/drive/MyDrive/Research/1bs_snr\"+str(snrdb)+\"dB_mdfa_predictions.npy\"\n","    xHt = np.load(name)\n","    xHt = xHt.T[:N,:]\n","    xHt = xHt.T\n","    label = '/content/drive/MyDrive/Research/1bs_snr'+str(snrdb)+'dB_mdfa_labels.npy'\n","    labels = np.load(label)\n","\n","    # find the avg MD FA\n","    fa_ar = np.zeros(test_to)\n","    md_ar = np.zeros(test_to)\n","    for i in range(test_to):\n","        user_guess = np.where(abs(xHt[i]) > threshold)\n","        real_users = np.where(abs(labels[i]) == 1)\n","        fa_ar[i] = len(np.setdiff1d(user_guess, real_users))\n","        md_ar[i] = len(np.setdiff1d(real_users, user_guess))\n","    # find the avg\n","    avg_fa.append(sum(fa_ar) / test_to)\n","    avg_md.append(sum(md_ar) / test_to)\n","  print(f'Threshold: {threshold}')\n","  print(f'FA: {avg_fa}')\n","  print(f'MD: {avg_md}')"]},{"cell_type":"code","source":["threshold = .4  # change me based on results above\n","\n","avg_fa = []\n","avg_md= []\n","# now test on the remaining 100 samples\n","for snrdb in snr_dB:\n","  name = \"/content/drive/MyDrive/Research/1bs_snr\"+str(snrdb)+\"dB_mdfa_predictions.npy\"\n","  xHt = np.load(name)\n","  xHt = xHt.T[:N,:]\n","  xHt = xHt.T\n","  label = '/content/drive/MyDrive/Research/1bs_snr'+str(snrdb)+'dB_mdfa_labels.npy'\n","  labels = np.load(label)\n","\n","  # find the avg MD FA\n","  fa_ar = np.zeros(100)\n","  md_ar = np.zeros(100)\n","  cnt = 0\n","  for i in range(test_to, num_test_samples):\n","    user_guess = np.where(abs(xHt[i]) > threshold)\n","    real_users = np.where(abs(labels[i]) == 1)\n","    fa_ar[cnt] = len(np.setdiff1d(user_guess, real_users))\n","    md_ar[cnt] = len(np.setdiff1d(real_users, user_guess))\n","    cnt+=1\n","  # find the avg\n","  avg_fa.append(sum(fa_ar) / 100)\n","  avg_md.append(sum(md_ar) / 100)\n","\n","print(f'Threshold: {threshold}')\n","print(f'FA: {avg_fa}')\n","print(f'MD: {avg_md}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3f2VnDQTDMZ","executionInfo":{"status":"ok","timestamp":1680193760823,"user_tz":300,"elapsed":9847,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"}},"outputId":"9d2fa3e7-2722-4fa3-e186-3b6cafc6d2ca"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold: 0.4\n","FA: [4.76, 3.6, 1.34, 0.43, 0.43, 0.2, 0.19]\n","MD: [15.93, 12.2, 6.09, 3.21, 2.87, 2.75, 1.74]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680063376495,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"},"user_tz":300},"id":"S070GtJERZ8X","outputId":"89403730-53c4-4f88-bac0-a3c0f5e1e8f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 BS. CRF. threshold: 0.7\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# plot the results\n","total_er =[]\n","for i in range(len(avg_fa)):\n","  total_er.append(avg_fa[i] + avg_md[i])\n","print(f\"1 BS. CRF. threshold: {threshold}\")\n","plt.figure()\n","plt.plot(snr_dB, avg_fa, label=\"FA - 1 BS\", linestyle=\"dashed\")\n","plt.plot(snr_dB, avg_md, label=\"MD - 1 BS\")\n","plt.plot(snr_dB, total_er, label='total errors - 1 BS', linestyle='dotted')\n","plt.legend()\n","plt.title(\"MD, FA vs. SNR\")\n","plt.xlabel(\"SNR(dB)\")\n","plt.ylabel(\"Number of MD, FA\")\n","plt.grid(True, which='both')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680063376496,"user":{"displayName":"Holly Roper","userId":"12327319719582283021"},"user_tz":300},"id":"SAgTdlBUfWBT","outputId":"ac3607c7-4a96-4ca6-9ed1-e8a00524e06c"},"outputs":[{"name":"stdout","output_type":"stream","text":["FA: [25.7142, 20.76508, 13.37024, 9.39456, 11.27152, 10.13448, 10.10408]\n","MD: [23.43748, 19.67324, 13.46308, 7.5124, 6.70592, 5.17644, 5.27004]\n"]}],"source":["print(f'FA: {avg_fa}')\n","print(f'MD: {avg_md}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRAckIkVkh1V"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"150DdoeKqsia7jMItMqzj8hFooy3Fwodi","timestamp":1680058387422}],"mount_file_id":"1E4gHr7lm78TxffbvI277JNJvcOI0L2QY","authorship_tag":"ABX9TyPqf6lWM31yz4dbZ4kNt/XK"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}